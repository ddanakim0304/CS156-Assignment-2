{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4903efe2",
   "metadata": {},
   "source": [
    "Path A: The Autoencoder Approach (Learned Features): The model figures out what's important by itself.\n",
    "Path B: The Manual Labeling Approach (Engineered Features): You, the human, tell the model what's important by labeling it.\n",
    "\n",
    "The best project will use the Manual Labeling approach as a baseline to prove how powerful the Autoencoder is\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acd41f02",
   "metadata": {},
   "source": [
    "That's an excellent strategic question. The choice of boss will have a significant impact on the difficulty of your project and the clarity of your results. An easier boss is definitely better, but \"easy\" for an AI has a different meaning than \"easy\" for a human.\n",
    "\n",
    "Here is a breakdown of boss recommendations, from best to worst, specifically for your imitation learning project.\n",
    "\n",
    "---\n",
    "\n",
    "### **Top Recommendation: Cagney Carnation**\n",
    "\n",
    "This is, without a doubt, your best choice. Cagney provides the ideal balance of simplicity and variety for this project.\n",
    "\n",
    "**Reasons Why Cagney is Perfect:**\n",
    "\n",
    "1.  **Fixed Arena & Camera:** The screen does not scroll. The camera is locked in place, and the ground platforms never move. This is a **massive** advantage. It means the background is static, making it much easier for the autoencoder to learn to ignore it and focus only on the moving elements (Cagney, projectiles, you).\n",
    "2.  **Clear, Telegraphed Attacks:** Cagney's attacks are visually distinct and have obvious tells.\n",
    "    *   *Face Lunge:* His face stretches out.\n",
    "    *   *Seed Gatling Gun:* He opens his petals and shoots seeds in clear patterns.\n",
    "    *   *Flying Acorns:* Acorns fly in predictable arcs.\n",
    "    *   This clarity means there is a strong, learnable correlation between \"what's on screen\" and \"what the player should do.\" Your model can learn rules like \"IF big face stretches, THEN dash.\"\n",
    "3.  **Consistent Player Position:** You spend almost the entire fight on the ground level, moving left and right. There's very little vertical platforming. This simplifies the \"state\" of the player, making your actions (jump, dash, shoot) more consistent and easier for the model to imitate.\n",
    "4.  **Manageable Final Phase:** The final phase with the thorny vines is visually distinct but still predictable. The patterns are regular, making it a solvable problem for the AI.\n",
    "\n",
    "**In short, the Cagney Carnation fight is a highly constrained and repeatable scientific experiment, which is exactly what you want for a machine learning project.**\n",
    "\n",
    "---\n",
    "\n",
    "### **Good Alternative: Goopy Le Grande**\n",
    "\n",
    "If you want a slightly different but still very manageable boss, Goopy is a strong second choice.\n",
    "\n",
    "**Reasons Why Goopy is a Good Choice:**\n",
    "\n",
    "1.  **Fixed Arena:** Just like Cagney, the camera is locked, and the ground is flat. This is a huge bonus.\n",
    "2.  **Simple Visuals:** Goopy is just a big blue blob. His attacks (bouncing, punching) are visually simple. There are very few projectiles on screen at any given time, reducing visual clutter.\n",
    "3.  **Distinct Phases:** His transformation into a giant Goopy and then a tombstone are visually dramatic and distinct. This is a great opportunity to analyze your autoencoder's latent space (the t-SNE plot) to see if it creates three separate \"mega-clusters\" corresponding to the three phases of the fight.\n",
    "\n",
    "**The Downside:** The fight is almost *too* simple. The required actions are less varied than in the Cagney fight (mostly just jumping over him), which might make the results slightly less interesting, but it's a very safe and reliable choice.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5809a690",
   "metadata": {},
   "source": [
    "# Sync check\n",
    "\n",
    "https://www.loom.com/share/2841d59aadc14c17be7281f7e3fc98c2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5879a2a5",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import os\n",
    "# Set Keras backend to TensorFlow\n",
    "os.environ[\"KERAS_BACKEND\"] = \"tensorflow\"\n",
    "\n",
    "from keras.layers import Input, Conv2D, MaxPooling2D, UpSampling2D, Conv2DTranspose\n",
    "from keras.models import Model\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "print(\"✅ Keras imports successful!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6bcf4e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating total number of valid fight frames...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff1fa03993d042c3bbb5f3d920fc277c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7222 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 6118 frames within fight intervals.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from tqdm.notebook import tqdm\n",
    "import math\n",
    "\n",
    "# --- Configuration (remains the same) ---\n",
    "SESSION_NAME = \"Cagney_1\"\n",
    "DATA_DIR = Path(\"../data/sessions\")\n",
    "VIDEO_PATH = DATA_DIR / f\"{SESSION_NAME}.mp4\"\n",
    "LOG_PATH = DATA_DIR / f\"{SESSION_NAME}.jsonl\"\n",
    "IMG_HEIGHT, IMG_WIDTH = 72, 128\n",
    "ACTIONS = ['Key.up', 'Key.down', 'Key.left', 'Key.right', 'Key.space', 'f', 'd', 'x', 'a']\n",
    "ACTION_MAP = {action: i for i, action in enumerate(ACTIONS)}\n",
    "NUM_ACTIONS = len(ACTIONS)\n",
    "\n",
    "# --- Helper functions (remain the same) ---\n",
    "def get_fight_intervals(log_path):\n",
    "    intervals = []\n",
    "    start_time = None\n",
    "    with open(log_path, 'r') as f:\n",
    "        for line in f:\n",
    "            data = json.loads(line)\n",
    "            if data.get('event') == 'marker':\n",
    "                if data['type'] == 'fight_start': start_time = data['t']\n",
    "                elif data['type'] == 'fight_end' and start_time is not None:\n",
    "                    intervals.append((start_time, data['t']))\n",
    "                    start_time = None\n",
    "    return intervals\n",
    "\n",
    "def get_key_state_timeline(log_path):\n",
    "    key_events = []\n",
    "    with open(log_path, 'r') as f:\n",
    "        for line in f:\n",
    "            data = json.loads(line)\n",
    "            if data.get('event') in ['keydown', 'keyup']:\n",
    "                key_events.append((data['t'], data['key'], data['event']))\n",
    "    key_events.sort(key=lambda x: x[0])\n",
    "    return key_events\n",
    "\n",
    "def get_keys_down_at_time(timeline, current_time):\n",
    "    keys_down = set()\n",
    "    for t, key, event in timeline:\n",
    "        if t > current_time: break\n",
    "        if event == 'keydown': keys_down.add(key)\n",
    "        elif event == 'keyup': keys_down.discard(key)\n",
    "    return keys_down\n",
    "\n",
    "# --- NEW: The Data Generator ---\n",
    "def data_generator(video_path, log_path, batch_size=32):\n",
    "    fight_intervals = get_fight_intervals(log_path)\n",
    "    key_timeline = get_key_state_timeline(log_path)\n",
    "    \n",
    "    cap = cv2.VideoCapture(str(video_path))\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    \n",
    "    while True: # Loop forever so the generator never terminates\n",
    "        frames_batch = []\n",
    "        labels_batch = []\n",
    "        \n",
    "        while len(frames_batch) < batch_size:\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                # Reached end of video, reset the capture to the beginning\n",
    "                cap.set(cv2.CAP_PROP_POS_FRAMES, 0)\n",
    "                continue\n",
    "\n",
    "            current_time = cap.get(cv2.CAP_PROP_POS_MSEC) / 1000.0\n",
    "            is_in_fight = any(start <= current_time <= end for start, end in fight_intervals)\n",
    "\n",
    "            if is_in_fight:\n",
    "                processed_frame = cv2.resize(frame, (IMG_WIDTH, IMG_HEIGHT))\n",
    "                processed_frame = cv2.cvtColor(processed_frame, cv2.COLOR_BGR_GRAY)\n",
    "                processed_frame = processed_frame / 255.0\n",
    "                frames_batch.append(processed_frame)\n",
    "\n",
    "                keys_down = get_keys_down_at_time(key_timeline, current_time)\n",
    "                label = np.zeros(NUM_ACTIONS, dtype=int)\n",
    "                for key in keys_down:\n",
    "                    if key in ACTION_MAP:\n",
    "                        label[ACTION_MAP[key]] = 1\n",
    "                labels_batch.append(label)\n",
    "\n",
    "        # Convert batch to numpy arrays and yield\n",
    "        X_batch = np.array(frames_batch).reshape(-1, IMG_HEIGHT, IMG_WIDTH, 1)\n",
    "        y_batch = np.array(labels_batch)\n",
    "        yield X_batch, y_batch\n",
    "        \n",
    "# --- Get Total Number of Fight Frames (for training steps calculation) ---\n",
    "def count_fight_frames(video_path, log_path):\n",
    "    fight_intervals = get_fight_intervals(log_path)\n",
    "    cap = cv2.VideoCapture(str(video_path))\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    count = 0\n",
    "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    for i in tqdm(range(total_frames)):\n",
    "        current_time = i / fps\n",
    "        if any(start <= current_time <= end for start, end in fight_intervals):\n",
    "            count += 1\n",
    "    cap.release()\n",
    "    return count\n",
    "\n",
    "print(\"Calculating total number of valid fight frames...\")\n",
    "# This will still take a minute, but uses almost no memory\n",
    "total_fight_frames = count_fight_frames(VIDEO_PATH, LOG_PATH) \n",
    "print(f\"Found {total_fight_frames} frames within fight intervals.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8e9c0317",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Step 1: Defining the Encoder Architecture ---\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'Input' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m--- Step 1: Defining the Encoder Architecture ---\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# --- Encoder ---\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m input_img \u001b[38;5;241m=\u001b[39m \u001b[43mInput\u001b[49m(shape\u001b[38;5;241m=\u001b[39m(IMG_HEIGHT, IMG_WIDTH, \u001b[38;5;241m1\u001b[39m), name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEncoder_Input\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInput Shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00minput_img\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# First convolutional block\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Input' is not defined"
     ]
    }
   ],
   "source": [
    "print(\"--- Step 1: Defining the Encoder Architecture ---\")\n",
    "\n",
    "# --- Encoder ---\n",
    "input_img = Input(shape=(IMG_HEIGHT, IMG_WIDTH, 1), name=\"Encoder_Input\")\n",
    "print(f\"Input Shape: {input_img.shape}\")\n",
    "\n",
    "# First convolutional block\n",
    "x = Conv2D(16, (3, 3), activation='relu', padding='same')(input_img)\n",
    "x = MaxPooling2D((2, 2), padding='same')(x)\n",
    "print(f\"Shape after 1st Conv+Pool block: {x.shape}\")\n",
    "\n",
    "# Second convolutional block\n",
    "x = Conv2D(32, (3, 3), activation='relu', padding='same')(x)\n",
    "x = MaxPooling2D((2, 2), padding='same')(x)\n",
    "print(f\"Shape after 2nd Conv+Pool block: {x.shape}\")\n",
    "\n",
    "# Third convolutional block\n",
    "x = Conv2D(64, (3, 3), activation='relu', padding='same')(x)\n",
    "encoded = MaxPooling2D((2, 2), padding='same', name=\"Latent_Vector\")(x) \n",
    "print(f\"Shape of final ENCODED representation (Latent Vector): {encoded.shape}\")\n",
    "\n",
    "print(\"\\n--- Step 2: Defining the Decoder Architecture ---\")\n",
    "\n",
    "# --- Decoder ---\n",
    "# First deconvolutional block\n",
    "x = Conv2DTranspose(64, (3, 3), activation='relu', padding='same')(encoded)\n",
    "x = UpSampling2D((2, 2))(x)\n",
    "print(f\"Shape after 1st Deconv+UpSample block: {x.shape}\")\n",
    "\n",
    "# Second deconvolutional block\n",
    "x = Conv2DTranspose(32, (3, 3), activation='relu', padding='same')(x)\n",
    "x = UpSampling2D((2, 2))(x)\n",
    "print(f\"Shape after 2nd Deconv+UpSample block: {x.shape}\")\n",
    "\n",
    "# Third deconvolutional block\n",
    "x = Conv2DTranspose(16, (3, 3), activation='relu', padding='same')(x)\n",
    "x = UpSampling2D((2, 2))(x)\n",
    "print(f\"Shape after 3rd Deconv+UpSample block: {x.shape}\")\n",
    "\n",
    "# Final output layer to reconstruct the image\n",
    "decoded = Conv2D(1, (3, 3), activation='sigmoid', padding='same', name=\"Decoder_Output\")(x)\n",
    "print(f\"Shape of final DECODED output: {decoded.shape}\")\n",
    "\n",
    "\n",
    "print(\"\\n--- Step 3: Assembling and Compiling Models ---\")\n",
    "\n",
    "# The full autoencoder model (trains both encoder and decoder)\n",
    "autoencoder = Model(input_img, decoded, name=\"Autoencoder\")\n",
    "print(\"✅ Autoencoder model assembled.\")\n",
    "\n",
    "# A separate model that is just the encoder part\n",
    "encoder = Model(input_img, encoded, name=\"Encoder\")\n",
    "print(\"✅ Encoder-only model assembled.\")\n",
    "\n",
    "# Compile the autoencoder for training\n",
    "autoencoder.compile(optimizer='adam', loss='mse')\n",
    "print(\"✅ Autoencoder compiled with 'adam' optimizer and 'mse' loss.\")\n",
    "\n",
    "\n",
    "print(\"\\n--- Final Model Summary ---\")\n",
    "# Print the model summary to verify the architecture\n",
    "autoencoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ada0760",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UsageError: Line magic function `%` not found.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# In your training and visualization cell\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# --- Create Generators for Training and Validation ---\n",
    "BATCH_SIZE = 32\n",
    "# These variables should have been calculated in your first cell.\n",
    "# A common split is 80/20 for train/validation.\n",
    "train_frames = int(total_fight_frames * 0.8)\n",
    "val_frames = total_fight_frames - train_frames\n",
    "\n",
    "steps_per_epoch = train_frames // BATCH_SIZE\n",
    "validation_steps = val_frames // BATCH_SIZE\n",
    "\n",
    "print(f\"Training on {train_frames} frames, validating on {val_frames} frames.\")\n",
    "print(f\"Steps per epoch: {steps_per_epoch}\")\n",
    "print(f\"Validation steps: {validation_steps}\")\n",
    "\n",
    "# Initialize the generators\n",
    "train_gen = data_generator(VIDEO_PATH, LOG_PATH, batch_size=BATCH_SIZE)\n",
    "val_gen = data_generator(VIDEO_PATH, LOG_PATH, batch_size=BATCH_SIZE)\n",
    "\n",
    "\n",
    "# --- Train the Autoencoder using the Generator ---\n",
    "# The autoencoder learns to reconstruct its own input, so the generator\n",
    "# works perfectly here. The labels (y_batch) are simply ignored.\n",
    "print(\"\\nStarting autoencoder training...\")\n",
    "history = autoencoder.fit(\n",
    "    train_gen,\n",
    "    steps_per_epoch=steps_per_epoch,\n",
    "    epochs=20, # 20 is a great starting point\n",
    "    validation_data=val_gen,\n",
    "    validation_steps=validation_steps\n",
    ")\n",
    "print(\"Autoencoder training complete.\")\n",
    "\n",
    "\n",
    "# --- Save the Encoder Model ---\n",
    "# We only need the encoder part for the next stage.\n",
    "# Using the .keras extension is the modern, recommended format.\n",
    "encoder.save('cuphead_encoder.keras')\n",
    "print(\"Encoder model saved to cuphead_encoder.keras\")\n",
    "\n",
    "\n",
    "# --- Visualize the Reconstructions ---\n",
    "# We can't use `X_val` because it doesn't exist.\n",
    "# Instead, we'll grab ONE batch from our validation generator to see how well it works.\n",
    "print(\"\\nGenerating sample reconstructions for visualization...\")\n",
    "X_val_sample, _ = next(val_gen) # The `_` ignores the labels\n",
    "\n",
    "# Use the trained autoencoder to predict (reconstruct) this sample batch\n",
    "decoded_imgs = autoencoder.predict(X_val_sample)\n",
    "\n",
    "# --- Plot the results ---\n",
    "n = 8  # Let's display 8 images for a good overview\n",
    "plt.figure(figsize=(18, 4))\n",
    "plt.suptitle(\"Autoencoder Reconstruction Quality\", fontsize=16)\n",
    "for i in range(n):\n",
    "    # Display original image\n",
    "    ax = plt.subplot(2, n, i + 1)\n",
    "    plt.imshow(X_val_sample[i].reshape(IMG_HEIGHT, IMG_WIDTH), cmap='gray')\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "    if i == 0:\n",
    "        ax.set_title(\"Original\", loc='left', fontsize=10, y=-0.4)\n",
    "\n",
    "\n",
    "    # Display reconstructed image\n",
    "    ax = plt.subplot(2, n, i + 1 + n)\n",
    "    plt.imshow(decoded_imgs[i].reshape(IMG_HEIGHT, IMG_WIDTH), cmap='gray')\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "    if i == 0:\n",
    "        ax.set_title(\"Reconstructed\", loc='left', fontsize=10, y=-0.4)\n",
    "\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.95])\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
